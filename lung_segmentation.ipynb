{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"**<h1>RSNA Pneumonia Detection Challenge<h1>**\nSociedade Beneficente de Senhoras - Hospital Sírio-Libanês - Brazil","metadata":{"_uuid":"4e52def28a73f1ca23e48d5b6495061bcf6ef966"}},{"cell_type":"markdown","source":"# **Contents**\n1. [Overview](#1.-Overview)\n1. [Data preparation](#2.-Data-preparation)\n1. [Segmentation training](#3.-Segmentation-training)\n1. [Results](#4.-Results)","metadata":{"_uuid":"828d2fadb694c84f41867478a9608b822949f6a7"}},{"cell_type":"markdown","source":"# 1. Overview\nThis notebook follows the work of [Kevin Mader](https://www.kaggle.com/kmader/training-u-net-on-tb-images-to-segment-lungs/notebook) for lung segmentation. Our motivation is to automatically identify lung opacities in chest x-rays for the [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/leaderboard). \n\nMedical Image Segmentation is the process of automatic detection of boundaries within images. In this exercise, we train a convolutional neural network with [U-Net](https://arxiv.org/abs/1505.04597) architecture, which training strategy relies on the strong use of data augmentation to improve the efficiency of available annotated samples.\n\nThe training is done with two chest x-rays datasets: [Montgomery County and Shenzhen Hospital](https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/). The Montgomery County dataset includes manually segmented lung masks, whereas Shenzhen Hospital dataset was manually segmented by [Stirenko et al](https://arxiv.org/abs/1803.01199). The lung segmentation masks were dilated to load lung boundary information within the training net and the images were resized to 512x512 pixels.","metadata":{"_uuid":"516c9b53d192813594987edec5c0552c4449607c"}},{"cell_type":"markdown","source":"# 2. Data preparation\nPrepare the input segmentation directory structure.","metadata":{"_uuid":"2392d90df7ff92794b4d5ef9c47c102b289fd724"}},{"cell_type":"code","source":"# !mkdir \\input\\segmentation\n# !mkdir \\input\\segmentation\\test\n# !mkdir \\input\\segmentation\\train\n# !mkdir \\input\\segmentation\\train\\augmentation\n# !mkdir \\input\\segmentation\\train\\image\n# !mkdir \\input\\segmentation\\train\\mask\n# !mkdir \\input\\segmentation\\train\\dilate","metadata":{"_uuid":"763240ce0dbdfb26510ded9b55b5bc94d400f74e","execution":{"iopub.status.busy":"2022-07-25T05:39:31.451358Z","iopub.execute_input":"2022-07-25T05:39:31.451661Z","iopub.status.idle":"2022-07-25T05:39:31.455719Z","shell.execute_reply.started":"2022-07-25T05:39:31.451605Z","shell.execute_reply":"2022-07-25T05:39:31.454949Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Import required Python libraries","metadata":{"_uuid":"e222153d13b6602792f447ed86401f3ee5d4965b"}},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\nfrom glob import glob\nfrom tqdm import tqdm","metadata":{"_uuid":"cb2b30651cf57582f7715325ea9519f04f4407d4","execution":{"iopub.status.busy":"2022-07-25T05:39:31.476093Z","iopub.execute_input":"2022-07-25T05:39:31.476318Z","iopub.status.idle":"2022-07-25T05:39:31.960787Z","shell.execute_reply.started":"2022-07-25T05:39:31.476273Z","shell.execute_reply":"2022-07-25T05:39:31.959808Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define appropriate constants for directory paths and training parameters","metadata":{"_uuid":"2f6aa09deb74f25f9937d16ef32ddc32a22d780c"}},{"cell_type":"code","source":"INPUT_DIR = os.path.join(\"..\", \"input\")\n\nSEGMENTATION_DIR = os.path.join(INPUT_DIR, \"segmentation\")\nSEGMENTATION_TEST_DIR = os.path.join(SEGMENTATION_DIR, \"test\")\nSEGMENTATION_TRAIN_DIR = os.path.join(SEGMENTATION_DIR, \"train\")\nSEGMENTATION_AUG_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"augmentation\")\nSEGMENTATION_IMAGE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"image\")\nSEGMENTATION_MASK_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"mask\")\nSEGMENTATION_DILATE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"dilate\")\nSEGMENTATION_SOURCE_DIR = os.path.join(INPUT_DIR, \\\n                                       \"pulmonary-chest-xray-abnormalities\")\n\nSHENZHEN_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \"ChinaSet_AllFiles\", \\\n                                  \"ChinaSet_AllFiles\")\nSHENZHEN_IMAGE_DIR = os.path.join(SHENZHEN_TRAIN_DIR, \"CXR_png\")\nSHENZHEN_MASK_DIR = os.path.join(INPUT_DIR, \"shcxr-lung-mask\", \"mask\", \"mask\")\n\nMONTGOMERY_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \\\n                                    \"Montgomery\", \"MontgomerySet\")\nMONTGOMERY_IMAGE_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \"CXR_png\")\nMONTGOMERY_LEFT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                        \"ManualMask\", \"leftMask\")\nMONTGOMERY_RIGHT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                         \"ManualMask\", \"rightMask\")\n\nDILATE_KERNEL = np.ones((15, 15), np.uint8)\n\nBATCH_SIZE=2\n\n#Prod\nEPOCHS=56\n\n#Desv\n#EPOCHS=16","metadata":{"_uuid":"5307ff8180fe09ccb769e1e9809e33061451c5e1","execution":{"iopub.status.busy":"2022-07-25T05:39:31.962470Z","iopub.execute_input":"2022-07-25T05:39:31.962749Z","iopub.status.idle":"2022-07-25T05:39:31.973834Z","shell.execute_reply.started":"2022-07-25T05:39:31.962703Z","shell.execute_reply":"2022-07-25T05:39:31.972975Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"1. Combine left and right lung segmentation masks of Montgomery chest x-rays\n1. Resize images to 512x512 pixels\n1. Dilate masks to gain more information on the edge of lungs\n1. Split images into training and test datasets\n1. Write images to /segmentation directory","metadata":{"_uuid":"8a8adbf02b8a5c0b014b4e833e48d089e4f89035"}},{"cell_type":"code","source":"montgomery_left_mask_dir = glob(os.path.join(MONTGOMERY_LEFT_MASK_DIR, '*.png'))\nmontgomery_test = montgomery_left_mask_dir[0:50]\nmontgomery_train= montgomery_left_mask_dir[50:]\n\nfor left_image_file in tqdm(montgomery_left_mask_dir):\n    base_file = os.path.basename(left_image_file)\n    image_file = os.path.join(MONTGOMERY_IMAGE_DIR, base_file)\n    right_image_file = os.path.join(MONTGOMERY_RIGHT_MASK_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    left_mask = cv2.imread(left_image_file, cv2.IMREAD_GRAYSCALE)\n    right_mask = cv2.imread(right_image_file, cv2.IMREAD_GRAYSCALE)\n    \n    image = cv2.resize(image, (512, 512))\n    left_mask = cv2.resize(left_mask, (512, 512))\n    right_mask = cv2.resize(right_mask, (512, 512))\n    \n    mask = np.maximum(left_mask, right_mask)\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (left_image_file in montgomery_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)\n    else:\n        filename, fileext = os.path.splitext(base_file)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_mask%s\" % (filename, fileext)), mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext)), mask_dilate)","metadata":{"_uuid":"b8e995d39dc50ae09aaca9fca60d5f01a4afa493","execution":{"iopub.status.busy":"2022-07-25T05:39:31.976593Z","iopub.execute_input":"2022-07-25T05:39:31.977234Z","iopub.status.idle":"2022-07-25T05:40:31.692748Z","shell.execute_reply.started":"2022-07-25T05:39:31.976843Z","shell.execute_reply":"2022-07-25T05:40:31.691959Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 138/138 [00:59<00:00,  2.41it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define some useful functions to display images with segmentation as overlays","metadata":{"_uuid":"d6f4b42dcde137249d107e754efa7c25087224d1"}},{"cell_type":"code","source":"def add_colored_dilate(image, mask_image, dilate_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    dilate_image_gray = cv2.cvtColor(dilate_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    dilate = cv2.bitwise_and(dilate_image, dilate_image, mask=dilate_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n    dilate_coord = np.where(dilate!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n    dilate[dilate_coord[0],dilate_coord[1],:] = [0,0,255]\n\n    ret = cv2.addWeighted(image, 0.7, dilate, 0.3, 0)\n    ret = cv2.addWeighted(ret, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef add_colored_mask(image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(image, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef diff_mask(ref_image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(ref_image, 0.7, mask, 0.3, 0)\n    return ret","metadata":{"_uuid":"46e63227a43918c9189e192402be274f5e1e4466","execution":{"iopub.status.busy":"2022-07-25T05:40:31.695274Z","iopub.execute_input":"2022-07-25T05:40:31.695751Z","iopub.status.idle":"2022-07-25T05:40:31.710070Z","shell.execute_reply.started":"2022-07-25T05:40:31.695693Z","shell.execute_reply":"2022-07-25T05:40:31.709222Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Show some Montgomery chest x-rays and its lung segmentation masks from training and test dataset to verify the procedure above. In merged image it is possible to see the difference between the dilated mask in blue and the original mask in red.","metadata":{"_uuid":"a5eb09219a9cb07e2bd6bc855304ad5fb805415f"}},{"cell_type":"code","source":"base_file = os.path.basename(montgomery_train[0])\n\nimage_file = os.path.join(SEGMENTATION_IMAGE_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_MASK_DIR, base_file)\ndilate_image_file = os.path.join(SEGMENTATION_DILATE_DIR, base_file)\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n                          \nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\n\naxs[0, 0].set_title(\"X-Ray\")\naxs[0, 0].imshow(image)\n\naxs[0, 1].set_title(\"Mask\")\naxs[0, 1].imshow(mask_image)\n\naxs[0, 2].set_title(\"Dilate\")\naxs[0, 2].imshow(dilate_image)\n\naxs[0, 3].set_title(\"Merged\")\naxs[0, 3].imshow(merged_image)\n\nbase_file = os.path.basename(montgomery_test[0])\nfilename, fileext = os.path.splitext(base_file)\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\ndilate_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext))\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n\naxs[1, 0].set_title(\"X-Ray\")\naxs[1, 0].imshow(image)\n\naxs[1, 1].set_title(\"Mask\")\naxs[1, 1].imshow(mask_image)\n\naxs[1, 2].set_title(\"Dilate\")\naxs[1, 2].imshow(dilate_image)\n\naxs[1, 3].set_title(\"Merged\")\naxs[1, 3].imshow(merged_image)","metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"_uuid":"51da123d5e86d7dce7dfb126d76d83cf90dd0154","execution":{"iopub.status.busy":"2022-07-25T05:49:13.228521Z","iopub.execute_input":"2022-07-25T05:49:13.228814Z","iopub.status.idle":"2022-07-25T05:49:13.263520Z","shell.execute_reply.started":"2022-07-25T05:49:13.228756Z","shell.execute_reply":"2022-07-25T05:49:13.262172Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-60437b236de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmask_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdilate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilate_image_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmerged_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_colored_dilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-a0df90e4fa6b>\u001b[0m in \u001b[0;36madd_colored_dilate\u001b[0;34m(image, mask_image, dilate_image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_colored_dilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmask_image_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdilate_image_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilate_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_image_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"markdown","source":"1. Resize Shenzhen Hospital chest x-ray images to 512x512 pixels\n1. Dilate masks to gain more information on the edge of lungs\n1. Split images into training and test datasets\n1. Write images to /segmentation directory","metadata":{"_uuid":"961855a0ef2989b675ad037a1c674acf53701396"}},{"cell_type":"code","source":"shenzhen_mask_dir = glob(os.path.join(SHENZHEN_MASK_DIR, '*.png'))\nshenzhen_test = shenzhen_mask_dir[0:50]\nshenzhen_train= shenzhen_mask_dir[50:]\n\nfor mask_file in tqdm(shenzhen_mask_dir):\n    base_file = os.path.basename(mask_file).replace(\"_mask\", \"\")\n    image_file = os.path.join(SHENZHEN_IMAGE_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n        \n    image = cv2.resize(image, (512, 512))\n    mask = cv2.resize(mask, (512, 512))\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (mask_file in shenzhen_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)\n    else:\n        filename, fileext = os.path.splitext(base_file)\n\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_mask%s\" % (filename, fileext)), mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext)), mask_dilate)","metadata":{"_uuid":"de6eed98634dca524e5f326b8635cced94b0ce9b","execution":{"iopub.status.busy":"2022-07-25T06:24:11.870855Z","iopub.execute_input":"2022-07-25T06:24:11.871187Z","iopub.status.idle":"2022-07-25T06:26:18.873751Z","shell.execute_reply.started":"2022-07-25T06:24:11.871131Z","shell.execute_reply":"2022-07-25T06:26:18.872803Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 566/566 [02:06<00:00,  4.09it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Show some Shenzhen Hospital chest x-rays and its lung segmentation masks from training and test dataset to verify the procedure above. In merged image it is possible to see the difference between the dilated mask in blue and the original mask in red.","metadata":{"_uuid":"9e6e9dbb064305877847351015aba026582f7bb3"}},{"cell_type":"code","source":"base_file = os.path.basename(shenzhen_train[0].replace(\"_mask\", \"\"))\n\nimage_file = os.path.join(SEGMENTATION_IMAGE_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_MASK_DIR, base_file)\ndilate_image_file = os.path.join(SEGMENTATION_DILATE_DIR, base_file)\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n                          \nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\n\naxs[0, 0].set_title(\"X-Ray\")\naxs[0, 0].imshow(image)\n\naxs[0, 1].set_title(\"Mask\")\naxs[0, 1].imshow(mask_image)\n\naxs[0, 2].set_title(\"Dilate\")\naxs[0, 2].imshow(dilate_image)\n\naxs[0, 3].set_title(\"Merged\")\naxs[0, 3].imshow(merged_image)\n\nbase_file = os.path.basename(shenzhen_test[0].replace(\"_mask\", \"\"))\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nfilename, fileext = os.path.splitext(base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\n\nfilename, fileext = os.path.splitext(base_file)\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\ndilate_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext))\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n\naxs[1, 0].set_title(\"X-Ray\")\naxs[1, 0].imshow(image)\n\naxs[1, 1].set_title(\"Mask\")\naxs[1, 1].imshow(mask_image)\n\naxs[1, 2].set_title(\"Dilate\")\naxs[1, 2].imshow(dilate_image)\n\naxs[1, 3].set_title(\"Merged\")\naxs[1, 3].imshow(merged_image)","metadata":{"_uuid":"e9923cd36413cc086db684d3a076cbba9b479778","execution":{"iopub.status.busy":"2022-07-25T06:26:24.749964Z","iopub.execute_input":"2022-07-25T06:26:24.750334Z","iopub.status.idle":"2022-07-25T06:26:24.802129Z","shell.execute_reply.started":"2022-07-25T06:26:24.750269Z","shell.execute_reply":"2022-07-25T06:26:24.801055Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-08fac676d71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmask_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdilate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilate_image_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmerged_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_colored_dilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-a0df90e4fa6b>\u001b[0m in \u001b[0;36madd_colored_dilate\u001b[0;34m(image, mask_image, dilate_image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_colored_dilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmask_image_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdilate_image_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilate_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_image_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"markdown","source":"Print the count of images and segmentation lung masks available to test and train the model","metadata":{"_uuid":"67e2abed6ba57198cd442763925af44b493569ec"}},{"cell_type":"code","source":"train_files = glob(os.path.join(SEGMENTATION_IMAGE_DIR, \"*.png\"))\ntest_files = glob(os.path.join(SEGMENTATION_TEST_DIR, \"*.png\"))\nmask_files = glob(os.path.join(SEGMENTATION_MASK_DIR, \"*.png\"))\ndilate_files = glob(os.path.join(SEGMENTATION_DILATE_DIR, \"*.png\"))\n\n(len(train_files), \\\n len(test_files), \\\n len(mask_files), \\\n len(dilate_files))","metadata":{"_uuid":"e5381719ef7e49e6823c1a9417e7eff7cfb1572e","execution":{"iopub.status.busy":"2022-07-25T06:26:52.093161Z","iopub.execute_input":"2022-07-25T06:26:52.093462Z","iopub.status.idle":"2022-07-25T06:26:52.102639Z","shell.execute_reply.started":"2022-07-25T06:26:52.093406Z","shell.execute_reply":"2022-07-25T06:26:52.101752Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(0, 0, 0, 0)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Segmentation training\n\nReferences: https://github.com/zhixuhao/unet/, https://github.com/jocicmarko/ultrasound-nerve-segmentation","metadata":{"_uuid":"774eec88428a8d1899247401acfef369e20abd2e"}},{"cell_type":"markdown","source":"Data augmentation helper function for training the net","metadata":{"_uuid":"ab2175b11034175361ed8595ef8715afd76fd04a"}},{"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\ndef train_generator(batch_size, train_path, image_folder, mask_folder, aug_dict,\n        image_color_mode=\"grayscale\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_directory(\n        train_path,\n        classes = [image_folder],\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_directory(\n        train_path,\n        classes = [mask_folder],\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = img / 255\n    mask = mask / 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","metadata":{"_uuid":"2e51bb0ff0d463ff5fc26af0d7bc0f3df745bdaa","execution":{"iopub.status.busy":"2022-07-25T06:26:57.993241Z","iopub.execute_input":"2022-07-25T06:26:57.993538Z","iopub.status.idle":"2022-07-25T06:26:58.004469Z","shell.execute_reply.started":"2022-07-25T06:26:57.993483Z","shell.execute_reply":"2022-07-25T06:26:58.003608Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"U-net architecture","metadata":{"_uuid":"11ad24573000715e344f314f9766867d8b228d45"}},{"cell_type":"code","source":"# From: https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef unet(input_size=(256,256,1)):\n    inputs = Input(input_size)\n    \n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"_uuid":"fd23bd3ff7c4ef8186043cfcc1a161b7b90ae893","execution":{"iopub.status.busy":"2022-07-25T06:27:06.206080Z","iopub.execute_input":"2022-07-25T06:27:06.206372Z","iopub.status.idle":"2022-07-25T06:27:06.231985Z","shell.execute_reply.started":"2022-07-25T06:27:06.206321Z","shell.execute_reply":"2022-07-25T06:27:06.230690Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Helper functions to load test chest x-ray images","metadata":{"_uuid":"a5af02b028aec42c9b1e9f9312114febe0df602c"}},{"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\ndef test_load_image(test_file, target_size=(256,256)):\n    img = cv2.imread(test_file, cv2.IMREAD_GRAYSCALE)\n    img = img / 255\n    img = cv2.resize(img, target_size)\n    img = np.reshape(img, img.shape + (1,))\n    img = np.reshape(img,(1,) + img.shape)\n    return img\n\ndef test_generator(test_files, target_size=(256,256)):\n    for test_file in test_files:\n        yield test_load_image(test_file, target_size)\n        \ndef save_result(save_path, npyfile, test_files):\n    for i, item in enumerate(npyfile):\n        result_file = test_files[i]\n        img = (item[:, :, 0] * 255.).astype(np.uint8)\n\n        filename, fileext = os.path.splitext(os.path.basename(result_file))\n\n        result_file = os.path.join(save_path, \"%s_predict%s\" % (filename, fileext))\n\n        cv2.imwrite(result_file, img)","metadata":{"_uuid":"00c16a2cd8fe3b47cddb7c4a7c1de31b5bae481c","execution":{"iopub.status.busy":"2022-07-25T06:27:12.701890Z","iopub.execute_input":"2022-07-25T06:27:12.702215Z","iopub.status.idle":"2022-07-25T06:27:12.712416Z","shell.execute_reply.started":"2022-07-25T06:27:12.702162Z","shell.execute_reply":"2022-07-25T06:27:12.710893Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Select test and validation files","metadata":{"_uuid":"c92b6dc486eefe890feec5fbd760cb766a3612f1"}},{"cell_type":"code","source":"def add_suffix(base_file, suffix):\n    filename, fileext = os.path.splitext(base_file)\n    return \"%s_%s%s\" % (filename, suffix, fileext)\n\ntest_files = [test_file for test_file in glob(os.path.join(SEGMENTATION_TEST_DIR, \"*.png\")) \\\n              if (\"_mask\" not in test_file \\\n                  and \"_dilate\" not in test_file \\\n                  and \"_predict\" not in test_file)]\n\nvalidation_data = (test_load_image(test_files[0], target_size=(512, 512)),\n                    test_load_image(add_suffix(test_files[0], \"dilate\"), target_size=(512, 512)))\n\nlen(test_files), len(validation_data)","metadata":{"_uuid":"09df4552b2a1c6935849214f4f439d2c581d8c16","execution":{"iopub.status.busy":"2022-07-25T06:27:22.380459Z","iopub.execute_input":"2022-07-25T06:27:22.380779Z","iopub.status.idle":"2022-07-25T06:27:22.400027Z","shell.execute_reply.started":"2022-07-25T06:27:22.380721Z","shell.execute_reply":"2022-07-25T06:27:22.397775Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-cb4a64f3dc38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                   and \"_predict\" not in test_file)]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m validation_data = (test_load_image(test_files[0], target_size=(512, 512)),\n\u001b[0m\u001b[1;32m     11\u001b[0m                     test_load_image(add_suffix(test_files[0], \"dilate\"), target_size=(512, 512)))\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"markdown","source":"Prepare the U-Net model and train the model. It will take a while...","metadata":{"_uuid":"f48364da322a5fd84178cac099510598e8c67f77"}},{"cell_type":"code","source":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\ntrain_gen = train_generator(BATCH_SIZE,\n                            SEGMENTATION_TRAIN_DIR,\n                            'image',\n                            'dilate', \n                            train_generator_args,\n                            target_size=(512,512),\n                            save_to_dir=os.path.abspath(SEGMENTATION_AUG_DIR))\n\nmodel = unet(input_size=(512,512,1))\nmodel.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, \\\n                  metrics=[dice_coef, 'binary_accuracy'])\nmodel.summary()\n\nmodel_checkpoint = ModelCheckpoint('unet_lung_seg.hdf5', \n                                   monitor='loss', \n                                   verbose=1, \n                                   save_best_only=True)\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=len(train_files) / BATCH_SIZE, \n                              epochs=EPOCHS, \n                              callbacks=[model_checkpoint],\n                              validation_data = validation_data)","metadata":{"_uuid":"9893f1eedda1d1fe36cfd042cc28144dd9d6b1f0","execution":{"iopub.status.busy":"2022-07-25T06:27:32.298974Z","iopub.execute_input":"2022-07-25T06:27:32.299271Z","iopub.status.idle":"2022-07-25T06:27:32.824647Z","shell.execute_reply.started":"2022-07-25T06:27:32.299217Z","shell.execute_reply":"2022-07-25T06:27:32.822022Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 512, 512, 32) 320         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 512, 512, 32) 9248        conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 32) 0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 256, 256, 64) 18496       max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 128, 128, 128 147584      conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 64, 64, 256)  524544      conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 64, 64, 512)  0           conv2d_transpose_1[0][0]         \n                                                                 conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 128, 128, 128 131200      conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 128, 128, 256 0           conv2d_transpose_2[0][0]         \n                                                                 conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 128, 128, 128 295040      concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 128, 128, 128 147584      conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTrans (None, 256, 256, 64) 32832       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 256, 256, 128 0           conv2d_transpose_3[0][0]         \n                                                                 conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 512, 512, 32) 8224        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 512, 512, 64) 0           conv2d_transpose_4[0][0]         \n                                                                 conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 512, 512, 32) 18464       concatenate_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 512, 512, 32) 9248        conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 512, 512, 1)  33          conv2d_18[0][0]                  \n==================================================================================================\nTotal params: 7,759,521\nTrainable params: 7,759,521\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-afc3d1134309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                               validation_data = validation_data)\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'validation_data' is not defined"],"ename":"NameError","evalue":"name 'validation_data' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Show some results from model fitting history","metadata":{"_uuid":"7fa6d4810998c47cca1d54005c5beb873cf3da8a"}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize = (15, 4))\n\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\ntraining_accuracy = history.history['binary_accuracy']\nvalidation_accuracy = history.history['val_binary_accuracy']\n\nepoch_count = range(1, len(training_loss) + 1)\n\naxs[0].plot(epoch_count, training_loss, 'r--')\naxs[0].plot(epoch_count, validation_loss, 'b-')\naxs[0].legend(['Training Loss', 'Validation Loss'])\n\naxs[1].plot(epoch_count, training_accuracy, 'r--')\naxs[1].plot(epoch_count, validation_accuracy, 'b-')\naxs[1].legend(['Training Accuracy', 'Validation Accuracy'])","metadata":{"_uuid":"925c24f71ba1c8e3697312ff7315e5e9f8059a0b","execution":{"iopub.status.busy":"2022-07-25T06:27:42.647341Z","iopub.execute_input":"2022-07-25T06:27:42.647628Z","iopub.status.idle":"2022-07-25T06:27:43.010635Z","shell.execute_reply.started":"2022-07-25T06:27:42.647577Z","shell.execute_reply":"2022-07-25T06:27:43.009482Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-1f679d64866c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"],"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x288 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA3IAAAD8CAYAAAAyhZbUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEdRJREFUeJzt3V+IpXd5B/DvY9ZYaq2W7gqSXZuUrtXFFrRDahGqRVs2udi9qJQEpFWCgbYppUohxaISr6zUQiGtrii2hRpTL8qAkVxoiiBGMmINJpIyTa3ZtJD1T3MjmqZ9enGO7XTczZyze2be88t8PrBw3vf8mPPwY3a/+53znnequwMAAMA4njP1AAAAACxHkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDB7FnkquqjVfVEVX31Es9XVf15VW1X1YNV9erVjwkA60dGAjCVRd6R+1iS08/w/A1JTs7/3JrkL698LAAYwsciIwGYwJ5Frrs/l+Tbz7DkbJK/7pn7k7yoql6yqgEBYF3JSACmcmQFX+OaJI/tOD4/P/fvuxdW1a2Z/UQyz3/+83/h5S9/+QpeHoB196Uvfemb3X1s6jkmICMBuKQrycdVFLmFdfe5JOeSZGNjo7e2tg7y5QGYSFX969QzrDsZCXD4XEk+ruKulY8nObHj+Pj8HAAcdjISgH2xiiK3meQ353fmek2SJ7v7hy4ZAYBDSEYCsC/2vLSyqj6e5PVJjlbV+STvTvLcJOnuDya5J8mNSbaTfDfJW/drWABYJzISgKnsWeS6++Y9nu8kv7uyiQBgEDISgKms4tJKAAAADpAiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwCxW5qjpdVY9U1XZV3X6R519aVfdV1Zer6sGqunH1owLAepGPAExlzyJXVVcluTPJDUlOJbm5qk7tWvbHSe7u7lcluSnJX6x6UABYJ/IRgCkt8o7c9Um2u/vR7n4qyV1Jzu5a00l+fP74hUn+bXUjAsBako8ATGaRIndNksd2HJ+fn9vpPUneXFXnk9yT5Pcu9oWq6taq2qqqrQsXLlzGuACwNlaWj4mMBGA5q7rZyc1JPtbdx5PcmORvquqHvnZ3n+vuje7eOHbs2IpeGgDW1kL5mMhIAJazSJF7PMmJHcfH5+d2uiXJ3UnS3V9I8iNJjq5iQABYU/IRgMksUuQeSHKyqq6rqqsz+7D25q4130jyhiSpqldkFlSuCwHg2Uw+AjCZPYtcdz+d5LYk9yb5WmZ333qoqu6oqjPzZe9I8raq+kqSjyd5S3f3fg0NAFOTjwBM6cgii7r7nsw+pL3z3Lt2PH44yWtXOxoArDf5CMBUVnWzEwAAAA6IIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwmIWKXFWdrqpHqmq7qm6/xJrfqKqHq+qhqvrb1Y4JAOtHPgIwlSN7Laiqq5LcmeRXk5xP8kBVbXb3wzvWnEzyR0le293fqaoX79fAALAO5CMAU1rkHbnrk2x396Pd/VSSu5Kc3bXmbUnu7O7vJEl3P7HaMQFg7chHACazSJG7JsljO47Pz8/t9LIkL6uqz1fV/VV1+mJfqKpuraqtqtq6cOHC5U0MAOthZfmYyEgAlrOqm50cSXIyyeuT3Jzkw1X1ot2Luvtcd29098axY8dW9NIAsLYWysdERgKwnEWK3ONJTuw4Pj4/t9P5JJvd/Z/d/S9J/imz4AKAZyv5CMBkFilyDyQ5WVXXVdXVSW5Ksrlrzd9n9tPGVNXRzC4leXSFcwLAupGPAExmzyLX3U8nuS3JvUm+luTu7n6oqu6oqjPzZfcm+VZVPZzkviR/2N3f2q+hAWBq8hGAKVV3T/LCGxsbvbW1NclrA3CwqupL3b0x9RyjkJEAh8OV5OOqbnYCAADAAVHkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxmoSJXVaer6pGq2q6q259h3a9XVVfVxupGBID1JB8BmMqeRa6qrkpyZ5IbkpxKcnNVnbrIuhck+f0kX1z1kACwbuQjAFNa5B2565Nsd/ej3f1UkruSnL3IuvcmeV+S761wPgBYV/IRgMksUuSuSfLYjuPz83P/q6peneREd3/qmb5QVd1aVVtVtXXhwoWlhwWANbKyfJyvlZEALOyKb3ZSVc9J8oEk79hrbXef6+6N7t44duzYlb40AKytZfIxkZEALGeRIvd4khM7jo/Pz/3AC5K8Msk/VNXXk7wmyaYPdAPwLCcfAZjMIkXugSQnq+q6qro6yU1JNn/wZHc/2d1Hu/va7r42yf1JznT31r5MDADrQT4CMJk9i1x3P53ktiT3Jvlakru7+6GquqOqzuz3gACwjuQjAFM6ssii7r4nyT27zr3rEmtff+VjAcD6k48ATOWKb3YCAADAwVLkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAazUJGrqtNV9UhVbVfV7Rd5/u1V9XBVPVhVn6mqn1r9qACwXuQjAFPZs8hV1VVJ7kxyQ5JTSW6uqlO7ln05yUZ3/3ySTyb5k1UPCgDrRD4CMKVF3pG7Psl2dz/a3U8luSvJ2Z0Luvu+7v7u/PD+JMdXOyYArB35CMBkFily1yR5bMfx+fm5S7klyacv9kRV3VpVW1W1deHChcWnBID1s7J8TGQkAMtZ6c1OqurNSTaSvP9iz3f3ue7e6O6NY8eOrfKlAWBt7ZWPiYwEYDlHFljzeJITO46Pz8/9P1X1xiTvTPK67v7+asYDgLUlHwGYzCLvyD2Q5GRVXVdVVye5KcnmzgVV9aokH0pyprufWP2YALB25CMAk9mzyHX300luS3Jvkq8lubu7H6qqO6rqzHzZ+5P8WJK/q6p/rKrNS3w5AHhWkI8ATGmRSyvT3fckuWfXuXftePzGFc8FAGtPPgIwlZXe7AQAAID9p8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGIwiBwAAMBhFDgAAYDCKHAAAwGAUOQAAgMEocgAAAINR5AAAAAajyAEAAAxGkQMAABiMIgcAADAYRQ4AAGAwihwAAMBgFDkAAIDBKHIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMIocAADAYBQ5AACAwShyAAAAg1HkAAAABqPIAQAADEaRAwAAGMxCRa6qTlfVI1W1XVW3X+T551XVJ+bPf7Gqrl31oACwbuQjAFPZs8hV1VVJ7kxyQ5JTSW6uqlO7lt2S5Dvd/TNJ/izJ+1Y9KACsE/kIwJQWeUfu+iTb3f1odz+V5K4kZ3etOZvkr+aPP5nkDVVVqxsTANaOfARgMkcWWHNNksd2HJ9P8ouXWtPdT1fVk0l+Msk3dy6qqluT3Do//H5VffVyhj6kjmbXfvKM7Ndy7Ndy7NfyfnbqAfbByvIxkZFXyN/J5div5div5div5Vx2Pi5S5Famu88lOZckVbXV3RsH+fojs1/LsV/LsV/LsV/Lq6qtqWdYdzLy8tmv5div5div5div5VxJPi5yaeXjSU7sOD4+P3fRNVV1JMkLk3zrcocCgAHIRwAms0iReyDJyaq6rqquTnJTks1dazaT/Nb88ZuSfLa7e3VjAsDakY8ATGbPSyvn1/TfluTeJFcl+Wh3P1RVdyTZ6u7NJB9J8jdVtZ3k25mF2V7OXcHch5H9Wo79Wo79Wo79Wt6zbs/2MR+TZ+F+7TP7tRz7tRz7tRz7tZzL3q/yg0EAAICxLPQLwQEAAFgfihwAAMBg9r3IVdXpqnqkqrar6vaLPP+8qvrE/PkvVtW1+z3TOltgv95eVQ9X1YNV9Zmq+qkp5lwXe+3XjnW/XlVdVYf6driL7FdV/cb8e+yhqvrbg55xnSzw9/GlVXVfVX15/nfyxinmXBdV9dGqeuJSv/+sZv58vp8PVtWrD3rGdSIflyMflycjlyMjlyMjF7dv+djd+/Ynsw9//3OSn05ydZKvJDm1a83vJPng/PFNST6xnzOt858F9+tXkvzo/PFv269n3q/5uhck+VyS+5NsTD33Ou9XkpNJvpzkJ+bHL5567jXfr3NJfnv++FSSr08998R79stJXp3kq5d4/sYkn05SSV6T5ItTzzzhXsnH1e+XfFxyz+brZOSC+yUjl94vGfl/e7Ev+bjf78hdn2S7ux/t7qeS3JXk7K41Z5P81fzxJ5O8oapqn+daV3vuV3ff193fnR/en9nvLTqsFvn+SpL3Jnlfku8d5HBraJH9eluSO7v7O0nS3U8c8IzrZJH96iQ/Pn/8wiT/doDzrZ3u/lxmd2a8lLNJ/rpn7k/yoqp6ycFMt3bk43Lk4/Jk5HJk5HJk5BL2Kx/3u8hdk+SxHcfn5+cuuqa7n07yZJKf3Oe51tUi+7XTLZm198Nqz/2avzV9ors/dZCDralFvr9eluRlVfX5qrq/qk4f2HTrZ5H9ek+SN1fV+ST3JPm9gxltWMv+G/dsJh+XIx+XJyOXIyOXIyNX67Lycc/fI8d6qqo3J9lI8rqpZ1lXVfWcJB9I8paJRxnJkcwuHXl9Zj/N/lxV/Vx3/8ekU62vm5N8rLv/tKp+KbPfF/bK7v7vqQeDw0o+LkZGXhYZuRwZuc/2+x25x5Oc2HF8fH7uomuq6khmb71+a5/nWleL7Feq6o1J3pnkTHd//4BmW0d77dcLkrwyyT9U1dczu+Z48xB/mHuR76/zSTa7+z+7+1+S/FNmoXUYLbJftyS5O0m6+wtJfiTJ0QOZbkwL/Rt3SMjH5cjH5cnI5cjI5cjI1bqsfNzvIvdAkpNVdV1VXZ3Zh7U3d63ZTPJb88dvSvLZnn/q7xDac7+q6lVJPpRZSB3ma7OTPfaru5/s7qPdfW13X5vZZybOdPfWNONObpG/j3+f2U8aU1VHM7uM5NGDHHKNLLJf30jyhiSpqldkFlIXDnTKsWwm+c353blek+TJ7v73qYeaiHxcjnxcnoxcjoxcjoxcrcvKx329tLK7n66q25Lcm9ndbT7a3Q9V1R1Jtrp7M8lHMnurdTuzDwHetJ8zrbMF9+v9SX4syd/NP/P+je4+M9nQE1pwv5hbcL/uTfJrVfVwkv9K8ofdfSjfAVhwv96R5MNV9QeZfaj7LYf4P9qpqo9n9p+co/PPRLw7yXOTpLs/mNlnJG5Msp3ku0neOs2k05OPy5GPy5ORy5GRy5GRy9mvfKxDup8AAADD2vdfCA4AAMBqKXIAAACDUeQAAAAGo8gBAAAMRpEDAAAYjCIHAAAwGEUOAABgMP8D2mYYIYez/XUAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Make lung segmentation predictions","metadata":{"_uuid":"7ac5b6dbe1dd70125529ffe46d748ddc2c4cc2a3"}},{"cell_type":"code","source":"test_gen = test_generator(test_files, target_size=(512,512))\nresults = model.predict_generator(test_gen, len(test_files), verbose=1)\nsave_result(SEGMENTATION_TEST_DIR, results, test_files)","metadata":{"_uuid":"15ad649aa0d5e94f42a78b58c8d58cd96d82a42d","execution":{"iopub.status.busy":"2022-07-25T05:40:31.970355Z","iopub.status.idle":"2022-07-25T05:40:31.971063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Results\n\nBelow, we see some results from our work, presented as Predicted, Gold Standard (manually segmented) and the difference between segmentations.\n\nThe next step will be the selection of lungs area on RSNA images dataset and the generation of a lungs-only image dataset.","metadata":{"_uuid":"067ce01e33264729319d4c3105e99e30e5a19163"}},{"cell_type":"code","source":"image = cv2.imread(\"../input/segmentation/test/CHNCXR_0003_0.png\")\npredict_image = cv2.imread(\"../input/segmentation/test/CHNCXR_0003_0_predict.png\")\nmask_image = cv2.imread(\"../input/segmentation/test/CHNCXR_0003_0_dilate.png\")\n\nfig, axs = plt.subplots(4, 3, figsize=(16, 16))\n\naxs[0, 0].set_title(\"Predicted\")\naxs[0, 0].imshow(add_colored_mask(image, predict_image))\naxs[0, 1].set_title(\"Gold Std.\")\naxs[0, 1].imshow(add_colored_mask(image, mask_image))\naxs[0, 2].set_title(\"Diff.\")\naxs[0, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"../input/segmentation/test/MCUCXR_0003_0.png\")\npredict_image = cv2.imread(\"../input/segmentation/test/MCUCXR_0003_0_predict.png\")\nmask_image = cv2.imread(\"../input/segmentation/test/MCUCXR_0003_0_dilate.png\")\n\naxs[1, 0].set_title(\"Predicted\")\naxs[1, 0].imshow(add_colored_mask(image, predict_image))\naxs[1, 1].set_title(\"Gold Std.\")\naxs[1, 1].imshow(add_colored_mask(image, mask_image))\naxs[1, 2].set_title(\"Diff.\")\naxs[1, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"../input/segmentation/test/CHNCXR_0020_0.png\")\npredict_image = cv2.imread(\"../input/segmentation/test/CHNCXR_0020_0_predict.png\")\nmask_image = cv2.imread(\"../input/segmentation/test/CHNCXR_0020_0_dilate.png\")\n\naxs[2, 0].set_title(\"Predicted\")\naxs[2, 0].imshow(add_colored_mask(image, predict_image))\naxs[2, 1].set_title(\"Gold Std.\")\naxs[2, 1].imshow(add_colored_mask(image, mask_image))\naxs[2, 2].set_title(\"Diff.\")\naxs[2, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"../input/segmentation/test/MCUCXR_0016_0.png\")\npredict_image = cv2.imread(\"../input/segmentation/test/MCUCXR_0016_0_predict.png\")\nmask_image = cv2.imread(\"../input/segmentation/test/MCUCXR_0016_0_dilate.png\")\n\naxs[3, 0].set_title(\"Predicted\")\naxs[3, 0].imshow(add_colored_mask(image, predict_image))\naxs[3, 1].set_title(\"Gold Std.\")\naxs[3, 1].imshow(add_colored_mask(image, mask_image))\naxs[3, 2].set_title(\"Diff.\")\naxs[3, 2].imshow(diff_mask(mask_image, predict_image))","metadata":{"_uuid":"3b1f04bd591e2dcfa01bfc239a52d574a75ca42b","execution":{"iopub.status.busy":"2022-07-25T05:40:31.972125Z","iopub.status.idle":"2022-07-25T05:40:31.972818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar zcf results.tgz --directory=../input/segmentation/test .","metadata":{"_uuid":"fc6cc9565c3f0df279dd1a79ba91c0b61d9e29fd","execution":{"iopub.status.busy":"2022-07-25T05:40:31.973926Z","iopub.status.idle":"2022-07-25T05:40:31.974747Z"},"trusted":true},"execution_count":null,"outputs":[]}]}